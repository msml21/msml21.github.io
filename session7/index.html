<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>PDEs and ODEs - MSML21</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="icon" type="image/png" href="/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&display=swap" rel="stylesheet">

  
  
  
  <link rel="stylesheet" href="/css/style.min.d59b5d72968250e157ade442f62f8c25566d6f41b28eb65800de4d89137ceede.css">
  

  

</head>

<body class='page page-default-single'>
  <div id="main-menu-mobile" class="main-menu-mobile">
  <ul>
    
    
    <li class="menu-item-conference program">
      <a href="/accepted/">
        <span>Conference Program</span>
      </a>
    </li>
    
    <li class="menu-item-key dates">
      <a href="/dates/">
        <span>Key Dates</span>
      </a>
    </li>
    
    <li class="menu-item-msml board">
      <a href="/board/">
        <span>MSML Board</span>
      </a>
    </li>
    
    <li class="menu-item-plenary speakers">
      <a href="/keynote/">
        <span>Plenary Speakers</span>
      </a>
    </li>
    
    <li class="menu-item-previous years">
      <a href="/previous/">
        <span>Previous Years</span>
      </a>
    </li>
    
    <li class="menu-item-program committee">
      <a href="/pc/">
        <span>Program Committee</span>
      </a>
    </li>
    
    <li class="menu-item-schedule">
      <a href="/schedule/">
        <span>Schedule</span>
      </a>
    </li>
    
    <li class="menu-item-workshops">
      <a href="/workshops/">
        <span>Workshops</span>
      </a>
    </li>
    
  </ul>
</div>
  <div class="wrapper">
    <div class='header'>
  <div class="container">
    <div class="logo">
      <a href="https://msml21.github.io/"><img height=48px alt="MSML" src="/images/msmllogo_flat.png" /></a>
    </div>
    <div class="logo-mobile">
      <a href="https://msml21.github.io/"><img height=24px alt="MSML" src="/images/msmllogo_flat.png" /></a>
    </div>
    <div id="main-menu" class="main-menu">
  <ul>
    
    
    <li class="menu-item-conference program">
      <a href="/accepted/">
        
        <span>Conference Program</span>
      </a>
	
    </li>
    
    <li class="menu-item-key dates">
      <a href="/dates/">
        
        <span>Key Dates</span>
      </a>
	
    </li>
    
    <li class="menu-item-msml board">
      <a href="/board/">
        
        <span>MSML Board</span>
      </a>
	
    </li>
    
    <li class="menu-item-plenary speakers">
      <a href="/keynote/">
        
        <span>Plenary Speakers</span>
      </a>
	
    </li>
    
    <li class="menu-item-previous years">
      <a href="/previous/">
        
        <span>Previous Years</span>
      </a>
	
    </li>
    
    <li class="menu-item-program committee">
      <a href="/pc/">
        
        <span>Program Committee</span>
      </a>
	
    </li>
    
    <li class="menu-item-schedule">
      <a href="/schedule/">
        
        <span>Schedule</span>
      </a>
	
    </li>
    
    <li class="menu-item-workshops">
      <a href="/workshops/">
        
        <span>Workshops</span>
      </a>
	
    </li>
    
  </ul>
</div>

    <button id="toggle-main-menu-mobile" class="hamburger hamburger--slider" type="button" aria-label="Mobile Menu">
  <span class="hamburger-box">
    <span class="hamburger-inner"></span>
  </span>
</button>
  </div>
</div>

    
<div class="container pb-6 pt-6 pt-md-5 pb-md-5">
  <div class="row justify-content-start">
    <div class="col-14 col-md-10">
      <h1 class="title">PDEs and ODEs</h1>
      <div class="content"><h2 id="chairs-ben-peherstorfer-nyu-and-nick-boffi-nyu">Chairs: Ben Peherstorfer (NYU) and Nick Boffi (NYU)</h2>
<h3 id="time-aug-19th-1220pm-110pm-et-1910-2000-cet-0110-0200-gmt8">Time: Aug 19th, 12:20pm-1:10pm ET, 19:10-20:00 CET, 01:10-02:00 GMT+8</h3>
<ul>
<li><strong>Some observations on partial differential equations in Barron and multi-layer spaces</strong>, Weinan E (Princeton University); Stephan Wojtowytsch (Princeton University)</li>
</ul>
<p><em>Paper Highlight, by Juncai He</em></p>
<blockquote>
<p>Barron and tree-like spaces are considered as appropriate function spaces to study the mathematical aspects of neural networks with one or multi hidden layers. This paper presents some observations about the Barron or tree-like regularities of solutions of three prototypical PDEs (screened Poisson, heat, and viscous HJB). From a mathematical perspective, the proof techniques are not difficult, but, some interesting results show that the Barron regularity may differ from the classical regularity theory of PDEs in certain aspects. Due to the increasing attempts of solving PDEs by neural networks, studying the regularity theory for PDEs under such function spaces plays a fundamental role in it.</p>
</blockquote>
<p><a href="../slides/id14.pdf">slides</a> <a href="https://drive.google.com/file/d/1IRK7xdsjhzpM_2Vj2e_b_QPDRiG5IR3A/view?usp=sharing">video</a> <a href="../papers/id14.pdf">paper</a></p>
<ul>
<li><strong>A deep learning method for solving Fokker-Planck equations</strong>, Yao Li (University of Massachusetts Amherst), Matthew Dobson (University of Massachusetts Amherst); Jiayu Zhai (University of Massachusetts Amherst)</li>
</ul>
<p><em>Paper Highlight, by Raffaele Marino</em></p>
<blockquote>
<p>The theory of Brownian motion, developed in different formulations by Einstein, Smoluchowski, and Langevin around 1905 and 1906, describes the dynamics of a particle suspended in a fluid. A prototypical example is a small colloidal object, e.g., a polystyrene bead about a micrometer in size, floating in the water at room temperature. Even without the action of externally applied forces, the particle is in an animated and erratic state of motion. This motion is generated at microscopic scales by collisions with the water molecules. It is visible at mesoscopic scales as an irregular diffusive movement. The Brownian motion has been successfully modeled by stochastic differential equations. Stochastic differential equations are used, in general, to model the dynamics of many real-world problems in the presence of uncertainty. The instantaneous and cumulative effects of the noise on the dynamics can be visualized through the probability distribution of the solution process. The Fokker-Planck equation (also known as the Kolmogorov forward equation) can analytically describe this probability measure. In general, this partial differential equation (PDE) cannot be solved analytically when the number of dimensions is high, and, therefore, numerical methods must be used. Traditional PDE solvers do not work well for the Fokker-Planck due to the curse of dimensionality and many other issues that high dimensional spaces bring together. However, recently, the application of deep learning methodology has shown many interesting results in solving PDEs in high-dimensional spaces. Deep learning is a class of machine learning algorithms that uses multiple layers to extract higher-level features from the raw input. It is based on Artificial Neural Networks, a series of functional transformations that can be obtained by fixing a set of basic functions in advance and allowing them to be adaptive during training. In this manuscript, the authors propose a mesh-free Fokker-Planck solver, in which a deep neural network represents the stationary solution to the Fokker-Planck equation and where just a small data set as a reference to locate the solution near the empirical probability distribution is needed. By introducing the differential operator of the Fokker-Planck equation into the loss function, the authors show improvements in the accuracy of the neural network representation with a reduction of the demand of data in the training process, reducing, therefore, the computational complexity for solving those kinds of PDE. Their simulations show that the neural network can tolerate very high noise in the training data long as it is spatially uncorrelated. This method, therefore, can help to deal with systems composed of many Brownian particles. It can be applied in many fields to understand the agents&rsquo; collective behavior comprising a complex system.</p>
</blockquote>
<p><a href="../slides/id48.pdf">slides</a> <a href="https://drive.google.com/file/d/1dP-yr4xc0M-A6fY-5AJrTdUJy4vAsc81/view?usp=sharing">video</a> <a href="../papers/id48.pdf">paper</a></p>
<ul>
<li><strong>Parameter Estimation with Dense and Convolutional Neural Networks Applied to the FitzHugh-Nagumo ODE</strong>, Johann Rudi (Argonne National Laboratory), Julie Bessac (Argonne National Laboratory); Amanda Lenzi (Argonne National Laboratory)</li>
</ul>
<p><em>Paper Highlight, by Zhizhen Zhao</em></p>
<blockquote>
<p>This paper discusses a parameter estimation problem for a specific system of ODEs, i.e., the FitzHughâ€“Nagumo equations that describe spiking neurons. The parameter estimation problem is
highly nonlinear. It is challenging and expensive to solve this inverse problem with the classical Bayesian framework. As such, the authors propose to use deep neural networks to learn a direct nonlinear
mapping from the measured time-series to the underlying ODE parameters. The paper has a very strong emphasis on the application and the authors provided an extensive analysis of results for simulated
clean and noisy observations. The authors compared CNNs with fully connected (dense) networks for parameter estimation. CNN architectures mostly show the lowest errors when recovering parameters, which can be attributed to their locally acting kernels being advantageous for time-evolving data. In addition, CNNs extract crucial properties or dynamics of the ODE output when predicting parameters from arbitrarily chosen partial observations; dense NNs, in contrast, perform significantly worse.</p>
</blockquote>
<p><a href="../slides/id54.pdf">slides</a> <a href="https://drive.google.com/file/d/1enOFjbkzyzyQm-cA4kUsrUSZyLcsM6_s/view?usp=sharing">video</a> <a href="../papers/id54.pdf">paper</a></p>
<ul>
<li><strong>Active learning with importance sampling: Optimizing objectives dominated by rare events to improve generalization</strong>, Grant M Rotskoff (Stanford University), Eric Vanden-Eijnden (New York University)</li>
</ul>
<p><em>Paper Highlight, by Lin Lin</em></p>
<blockquote>
<p>The authors introduce an approach that combines rare events sampling techniques with neural network optimization to optimize objective functions that are dominated by rare events. This is a variance reduction technique, which evaluates the gradient of the loss function based on a partition of unity type of construction. The online construction of these windowing functions enables the adaptive sampling of regions of interest. Numerical experiments indicate that the method can be used to successfully solve high dimensional partial differential equations.</p>
</blockquote>
<p><a href="../slides/id55.pdf">slides</a> <a href="https://drive.google.com/file/d/1v1iWPNv1Ual5KtrMCTtLzX_ELg6ekEkJ/view?usp=sharing">video</a> <a href="../papers/id55.pdf">paper</a></p>
<ul>
<li><strong>A semigroup method for high dimensional committor functions based on neural network</strong>, Haoya Li (Stanford University), Yuehaw Khoo (U Chicago); Yinuo Ren (Peking University); Lexing Ying (Stanford University)</li>
</ul>
<p><em>Paper highlight, by Jiequn Han</em></p>
<blockquote>
<p>This paper proposes a new method based on neural networks to compute the high-dimensional committor functions. Understanding transition dynamics from the commitor function is a fundamental problem in statistical mechanics with decades of work behind it. Traditional numerical methods have an intrinsic limitation in solving general high-dimensional commitor functions. Algorithms based on neural networks have received much interest in the community, all based on the Fokker-Planck equation&rsquo;s variational form. This paper&rsquo;s main innovation lies in proposing a new variational formulation (loss function) based on the differential operator&rsquo;s semigroup. The new formulation does not contain any differential operator, and the authors explicitly derive the loss&rsquo;s graidents used for the training. The gradients only involve the first-order derivatives of the neural networks, in contrast to the second-order derivatives required in the previous methods. This feature is conceptually beneficial to the efficient training of neural networks. Numerical results on the standard testing examples and the Ginzburg-Landau model demonstrate the superiority of the proposed method. Besides, the authors also show that in the lazy training regime, the corresponding gradient flow converges at a geometric rate to a local minimum under certain assumptions.</p>
</blockquote>
<p><a href="../slides/id59.pdf">slides</a> <a href="https://drive.google.com/file/d/1DtV2ku7ZAPJyeyqbdeHL2M1ZSyzAjAmV/view?usp=sharing">video</a> <a href="../papers/id59.pdf">paper</a></p>
</div>
    </div>
  </div>
</div>

  </div>

  <div class="sub-footer">
  <div class="container">
    <div class="row">
      <div class="col-12">
        <div class="sub-footer-inner">
          
            
<div class="social">
    
        <a href="mailto:msml21chairs@gmail.com" target="blank"><img src="/images/PICOL_icon_Mailw.png" title="Email" alt="Email" /></a>
    
        <a href="https://twitter.com/MsmlConference" target="blank"><img src="/images/twitter.svg" title="Twitter" alt="Twitter" /></a>
    
</div>

          
          
            <div class="copyright">Hugo design adapted from <a class="zerostatic" href="https://www.zerostatic.io">www.zerostatic.io</a> by Joan Bruna, 2020.</div>
          
        </div>
      </div>
    </div>
  </div>
</div>


  

  
  

  
  <script type="text/javascript" src="/js/scripts.min.bf1e1f7ae8e03db5f012356e825843facdff51c0a559cb0d27fe2bbe1db405c2.js"></script>
  

  






  





</body>

</html>
